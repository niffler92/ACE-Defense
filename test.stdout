G: None
alpha: 0.5
attack: None
autoencoder: False
batch_size: 128
ckpt_ae: None
ckpt_name: None
ckpt_src: None
conv_weight_init: xavier_normal
cuda: True
dataset: MNIST
defense: None
domain_restrict: False
download: False
eot_attack: pgd
eot_iter: 1000
eot_norm: l2
epochs: 100
gamma: 0.1
half: False
img_log_step: 20
lambd: 0
learning_rate: 0.01
log_step: 50
max_iter: 50
mode: train
model: LeNet_toy
momentum: 0.9
multigpu: 0
ndeflection: 200
no_cuda: False
nsamples: 30
num_classes: 10
optimizer: sgd
pad_type: replication
pretrained: False
random: False
seed: 500
sigma: 0.04
source: None
start_epoch: 0
target: None
verbose: 1
weight_decay: 0.0005
window: 10
workers: 4
x_coord: 1
y_coord: 0
<class 'submodules.models.lenet.LeNet_toy'>


    def __init__(self, args, **kwargs):
        super(LeNet_toy, self).__init__()
        if args.dataset == "MNIST":
            conv_in = 1
            linear_in = 1024
        else:
            conv_in = 3
            if args.dataset in ["CIFAR10", "CIFAR100"]:
                linear_in = 1600
            elif args.dataset == "TinyImageNet":
                linear_in = 1600*2*2
            elif args.dataset == "ImageNet":
                linear_in = 1600*7*7
            else:
                raise NotImplementedError

        self.conv1 = nn.Conv2d(conv_in, 16, 3, stride=2)
        self.conv2 = nn.Conv2d(16, 32, 3, stride=2)
        self.conv3 = nn.Conv2d(32, 64, 3, stride=1)

        self.fc1   = nn.Linear(linear_in, 512)
        self.fc2   = nn.Linear(512, args.num_classes)

        self.activation1 = nn.ReLU()
        self.activation2 = nn.ReLU()
        self.activation3 = nn.ReLU()
        self.activation4 = nn.ReLU()

        self.grads = {}
        self.get_grad = False
        self.endpoints = {}



    def forward(self, x):
        conv1 = self.activation1(self.conv1(x))
        conv2 = self.activation2(self.conv2(conv1))
        conv3 = self.activation3(self.conv3(conv2))
        out = conv3.view(conv3.size(0), -1)
        fc1 = self.activation4(self.fc1(out))
        out = self.fc2(fc1)

        if x.requires_grad:
            x.register_hook(self.save_grad('input'))
            conv1.register_hook(self.save_grad('conv1'))
            conv2.register_hook(self.save_grad('conv2'))
            conv3.register_hook(self.save_grad('conv3'))

            self.endpoints['input'] = x
            self.endpoints['conv1'] = conv1
            self.endpoints['conv2'] = conv2
            self.endpoints['conv3'] = conv3

        return out



**************************************** LeNet_toy*********************************************
****************************************PARAM INFO*********************************************
-----------------------------------------------------------------------------------------------
|                               Param Name |                     Shape |     Number of Params |
-----------------------------------------------------------------------------------------------
|                             conv1.weight |         [16L, 1L, 3L, 3L] |                  144 |
|                               conv1.bias |                     [16L] |                   16 |
|                             conv2.weight |        [32L, 16L, 3L, 3L] |                 4608 |
|                               conv2.bias |                     [32L] |                   32 |
|                             conv3.weight |        [64L, 32L, 3L, 3L] |                18432 |
|                               conv3.bias |                     [64L] |                   64 |
|                               fc1.weight |             [512L, 1024L] |               524288 |
|                                 fc1.bias |                    [512L] |                  512 |
|                               fc2.weight |               [10L, 512L] |                 5120 |
|                                 fc2.bias |                     [10L] |                   10 |
-----------------------------------------------------------------------------------------------
Total Params: 553226
***********************************************************************************************
